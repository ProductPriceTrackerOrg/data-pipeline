"""
LifeMobile Web Scraper - Fresh Data Pipeline
Scrapes product data from LifeMobile.lk and uploads to Azure Data Lake Storage (ADLS)

Usage:
    python main.py
"""

import logging
import sys
import os
import json
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add the current directory to the path to allow imports
sys.path.append('.')

# Azure Storage imports
try:
    from azure.storage.blob import BlobServiceClient
    AZURE_AVAILABLE = True
except ImportError:
    AZURE_AVAILABLE = False
    print("âš ï¸  Azure SDK not installed. Run: pip install azure-storage-blob")

from scripts.product_scraper_manager import LifeMobileScrapingManager, setup_scraping_environment


def print_banner():
    """Print application banner"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    LifeMobile Web Scraper                    â•‘
â•‘                  Fresh Data Pipeline                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


def clean_existing_data():
    """Remove existing JSON files to ensure fresh data"""
    json_files = [
        "lifemobile_products.json",
        "lifemobile_products_fixed.json", 
        "lifemobile_products_backup.json"
    ]
    
    cleaned_count = 0
    for file_path in json_files:
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                cleaned_count += 1
                print(f"ğŸ—‘ï¸  Removed: {file_path}")
            except Exception as e:
                print(f"âš ï¸  Could not remove {file_path}: {e}")
    
    if cleaned_count > 0:
        print(f"âœ… Cleaned {cleaned_count} existing data files")
    else:
        print("ğŸ“‚ No existing data files to clean")


def upload_to_adls(json_data: str, source_website: str):
    """
    Uploads a JSON string to ADLS as a single JSON file.
    
    Args:
        json_data: Ready-to-upload JSON string with properly serialized data
        source_website: Name of the source website (used for partitioning)
    """
    if not AZURE_AVAILABLE:
        print("âŒ Azure SDK not available. Cannot upload to ADLS.")
        return False
    
    # Get Azure Connection String from Environment Variable
    connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
    if not connection_string:
        print("âŒ Azure connection string not found in environment variables.")
        print("ğŸ’¡ Please set AZURE_STORAGE_CONNECTION_STRING in your .env file")
        return False

    # Define the partitioned path
    scrape_date = datetime.now().strftime('%Y-%m-%d')
    file_path = f"source_website={source_website}/scrape_date={scrape_date}/data.json"
    container_name = "raw-data"

    try:
        # Connect to Azure and Upload
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_path)

        print(f"â˜ï¸ Uploading data to: {container_name}/{file_path}")
        
        # Upload the JSON string
        blob_client.upload_blob(json_data, overwrite=True)

        print("âœ… Upload to Azure Data Lake Storage completed successfully!")
        print(f"ğŸ“ Location: {container_name}/{file_path}")
        return True

    except Exception as e:
        print(f"âŒ ADLS upload error: {e}")
        logging.error(f"ADLS upload error: {e}", exc_info=True)
        return False


def load_scraped_products(json_path="lifemobile_products.json"):
    """
    Load scraped products from the local JSON file with comprehensive error handling
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"{json_path} not found. Scraper did not produce output.")
    
    try:
        # First try normal JSON loading
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        # Validate data structure
        if not isinstance(data, list):
            raise ValueError(f"Expected list of products, got {type(data)}")
            
        # Check if items are dictionaries (not strings)
        dict_count = sum(1 for item in data if isinstance(item, dict))
        string_count = sum(1 for item in data if isinstance(item, str))
        
        print(f"ğŸ“Š Data type check: {type(data)}")
        if len(data) > 0:
            print(f"ğŸ“Š First item type: {type(data[0])}")
            if isinstance(data[0], str):
                print(f"ğŸ“Š First item preview: {data[0][:50]}...")
        
        if string_count > 0:
            print(f"ğŸ”§ Data contains strings instead of objects, attempting to parse...")
            # Try to parse string items as JSON
            fixed_data = []
            for item in data:
                if isinstance(item, str):
                    try:
                        parsed_item = json.loads(item)
                        fixed_data.append(parsed_item)
                    except json.JSONDecodeError:
                        continue  # Skip invalid items
                elif isinstance(item, dict):
                    fixed_data.append(item)
            
            if len(fixed_data) > 0:
                print(f"âœ… Successfully parsed {len(fixed_data)} valid products from {len(data)} total items")
                return fixed_data
            else:
                raise ValueError("âŒ Failed to parse string data: No valid JSON objects found")
        
        print(f"âœ… Data quality check passed: {dict_count} dictionary objects, {string_count} string objects")
        return data
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON parsing error: {e}")
        raise Exception(f"Unable to parse JSON file: {e}")


def main():
    """Main entry point - scrape fresh products and upload to Azure"""
    print_banner()
    print("ğŸš€ Starting LifeMobile fresh data pipeline...")
    print(f"â° Pipeline started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Setup environment and logging
        setup_scraping_environment()
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('lifemobile_scraper.log'),
                logging.StreamHandler()
            ]
        )
        
        # Step 0: Clean existing data files
        print("\nğŸ§¹ Step 0: Cleaning existing data files...")
        clean_existing_data()
        
        # Step 1: Run fresh scraping
        print("\nğŸ•·ï¸ Step 1: Scraping fresh products from LifeMobile.lk...")
        start_time = datetime.now()
        
        manager = LifeMobileScrapingManager()
        scraping_success = manager.run_scraper()
        
        if not scraping_success:
            print("âŒ Scraping failed!")
            return 1
        
        scraping_duration = (datetime.now() - start_time).total_seconds()
        print(f"âœ… Scraping completed in {scraping_duration:.2f} seconds")
        
        # Step 2: Load scraped products and remove JSON file immediately
        print("\nğŸ“‚ Step 2: Loading scraped data...")
        products_data = load_scraped_products("lifemobile_products.json")
        
        # Immediately remove the JSON file after loading
        try:
            if os.path.exists("lifemobile_products.json"):
                os.remove("lifemobile_products.json")
                print("ğŸ—‘ï¸ Removed temporary JSON file from directory")
        except Exception as e:
            print(f"âš ï¸ Could not remove JSON file: {e}")
        
        if not products_data:
            print("âŒ No valid products found in scraped data")
            return 1
        
        # Step 3: Data quality validation
        print("\nğŸ” Step 3: Data quality validation...")
        total_products = len(products_data)
        total_variants = sum(len(product.get('variants', [])) for product in products_data if isinstance(product, dict))
        scraping_speed = total_products / scraping_duration if scraping_duration > 0 else 0
        
        # Validate data quality
        dict_objects = sum(1 for item in products_data if isinstance(item, dict))
        string_objects = sum(1 for item in products_data if isinstance(item, str))
        
        print(f"ğŸ“Š Data Quality Check:")
        print(f"   Dictionary objects: {dict_objects}")
        print(f"   String objects: {string_objects}")
        print(f"   Scraping speed: {scraping_speed:.2f} products/second")
        
        if string_objects > 0:
            print("âŒ Data quality check failed - contains string objects")
            print("ğŸ’¡ Please fix the data format before uploading")
            return 1
        else:
            print("âœ… Data quality check passed - all objects are properly formatted")
        
        # Display scraping results
        print(f"\n{'='*60}")
        print(" âœ… SCRAPING COMPLETED SUCCESSFULLY")
        print(f"{'='*60}")
        print(f" ğŸ“¦ Products scraped: {total_products:,}")
        print(f" ğŸ·ï¸ Variants scraped: {total_variants:,}")
        print(f" â±ï¸ Scraping time: {scraping_duration:.2f} seconds")
        print(f" ğŸš€ Scraping speed: {scraping_speed:.2f} products/sec")
        
        # Step 4: Upload data to Azure Data Lake Storage
        if AZURE_AVAILABLE:
            try:
                print(f"\n{'='*60}")
                print(" â˜ï¸ UPLOADING TO AZURE DATA LAKE STORAGE")
                print(f"{'='*60}")
                
                # Handle datetime serialization
                def datetime_handler(obj):
                    if isinstance(obj, datetime):
                        return obj.isoformat()
                    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
                
                # Convert to JSON string with proper datetime handling
                json_data = json.dumps(products_data, indent=2, default=datetime_handler, ensure_ascii=False)
                
                # Validate JSON before upload
                try:
                    json.loads(json_data)
                    print("âœ… JSON validation passed")
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON validation failed: {e}")
                    return 1
                
                # Upload to ADLS
                upload_success = upload_to_adls(json_data=json_data, source_website="lifemobile.lk")
                
                if upload_success:
                    print("ğŸ‰ Data successfully uploaded to Azure Data Lake Storage!")
                    print(f"ğŸ“Š Final Summary:")
                    print(f"   ğŸ“¦ Products uploaded: {total_products:,}")
                    print(f"   ğŸ·ï¸ Variants uploaded: {total_variants:,}")
                    print(f"   ğŸ’¾ Data size: {len(json_data) / (1024*1024):.2f} MB")
                    print(f"   â±ï¸ Total time: {(datetime.now() - start_time).total_seconds():.2f} seconds")
                else:
                    print("âŒ Failed to upload data to ADLS")
                    
            except Exception as e:
                print(f"âŒ Failed to upload to ADLS: {e}")
                logging.error(f"ADLS upload error: {e}", exc_info=True)
        else:
            print("\nâš ï¸  Azure SDK not available - skipping upload")
            print("ğŸ’¡ Install with: pip install azure-storage-blob")

        # Final cleanup
        try:
            cleanup_files = ["lifemobile_products.json", "lifemobile_products_fixed.json"]
            for cleanup_file in cleanup_files:
                if os.path.exists(cleanup_file):
                    os.remove(cleanup_file)
                    print(f"ğŸ§¹ Final cleanup: Removed {cleanup_file}")
        except Exception as e:
            print(f"âš ï¸ Final cleanup warning: {e}")

        print(f"\nğŸŠ Pipeline completed successfully!")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Pipeline interrupted by user.")
        return 1
    except Exception as e:
        print(f"\nâŒ Pipeline failed: {e}")
        logging.error(f"Fatal error: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nOperation interrupted by user.")
        exit(1)
    except Exception as e:
        print(f"\nFatal error: {e}")
        exit(1)